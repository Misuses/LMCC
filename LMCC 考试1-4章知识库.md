# LMCC考试1-4章知识库
## 第一章 人工智能基础概念
### 一、人工智能相关概念定义
#### 核心知识点扩展
- **智能**：个体对环境的感知、理解、决策与执行能力，能通过学习适应复杂场景并解决问题。
- **人工智能（AI）**：通过计算机系统模拟人类智能的技术与学科，涵盖感知、推理、学习、决策等核心能力。
- **机器学习（ML）**：AI的核心分支，让计算机通过数据学习规律，无需显式编程即可完成任务。
- **有监督学习**：基于标注数据（输入+输出）训练模型，如分类（预测类别）、回归（预测连续值）。
- **无监督学习**：基于无标注数据挖掘潜在规律，如聚类（按相似性分组）、降维（简化数据维度）。
- **模型**：描述数据规律的数学框架（如神经网络、决策树），通过训练优化参数以拟合数据。
- **训练**：将数据输入模型，通过优化算法调整参数，使模型预测结果接近真实值的过程。

#### 例题及答案
**例题1（概念题）**：以下属于无监督学习任务的是（  ）
A. 预测商品价格  B. 识别手写数字  C. 客户分群  D. 判断邮件是否为垃圾邮件
**答案**：C  
解析：客户分群是聚类任务，属于无监督学习；A是回归任务，B、D是分类任务，均为有监督学习。

**例题2（简答题）**：简述人工智能与机器学习的关系。
**答案**：机器学习是人工智能的核心分支与实现方式之一，人工智能是更广泛的概念（涵盖规则式系统、机器学习等）；机器学习通过数据驱动让AI系统具备自主学习能力，是当前AI技术快速发展的核心动力。

### 二、机器学习流程及经典模型
#### 核心知识点扩展
- **数据预处理**：数据建模前的关键步骤，目的是提升数据质量，包括数据清洗、数据变换、数据划分。
- **数据清洗**：处理缺失值（删除、填充）、异常值（检测并修正/删除）、重复值（去重）。
- **数据变换**：标准化（均值为0、方差为1）、归一化（缩放到[0,1]区间）、编码（分类数据转数值，如独热编码）。
- **数据工程**：涵盖数据采集、清洗、变换、存储、特征提取的全流程，是机器学习成功的基础。
- **模型训练**：将预处理后的数据输入模型，通过优化目标函数（如损失函数）调整参数。
- **验证**：用验证集评估模型性能，调整超参数（如学习率、批次大小），避免过拟合。
- **测试**：用独立测试集最终评估模型泛化能力，确保模型在新数据上的有效性。
- **神经网络**：模拟人脑神经元结构的模型，由输入层、隐藏层、输出层组成，通过激活函数引入非线性，可拟合复杂数据规律。

#### 例题及答案
**例题1（实操题）**：已知某数据集包含缺失值（占比5%）和异常值（通过箱线图检测），请设计数据预处理流程。
**答案**：
1. 数据清洗：缺失值采用中位数填充（适用于数值型数据），异常值通过IQR法则（Q1-1.5IQR至Q3+1.5IQR）修正为边界值；
2. 数据变换：对数值型特征进行标准化处理，分类特征采用独热编码；
3. 数据划分：按7:2:1比例划分为训练集、验证集、测试集。

**例题2（代码实现题）**：用Python实现简单的数据标准化（基于NumPy）。
```python
import numpy as np

def standardize(data):
    # data为二维数组（行：样本，列：特征）
    mean = np.mean(data, axis=0)  # 各特征均值
    std = np.std(data, axis=0)    # 各特征标准差
    return (data - mean) / std    # 标准化后数据

# 测试
data = np.array([[1, 2], [3, 4], [5, 6]])
print(standardize(data))
```
**答案输出**：
```
[[-1.22474487 -1.22474487]
 [ 0.          0.        ]
 [ 1.22474487  1.22474487]]
```

### 三、验证及评测
#### 核心知识点扩展
- **交叉验证**：将训练集划分为k个子集，轮流用k-1个子集训练、1个子集验证，最终取平均性能，避免单次划分的偶然性（常用k=5或10）。
- **精确率（Precision）**：预测为正类的样本中，实际为正类的比例，公式：Precision = TP / (TP + FP)（TP：真阳性，FP：假阳性）。
- **召回率（Recall）**：实际为正类的样本中，被正确预测为正类的比例，公式：Recall = TP / (TP + FN)（FN：假阴性）。
- **ROC曲线**：以假阳性率（FPR=FP/(FP+TN)）为横轴、真阳性率（TPR=Recall）为纵轴的曲线，曲线下面积（AUC）越大，模型区分能力越强。
- **过拟合**：模型在训练集上性能优异，但在验证集/测试集上性能下降，原因是模型过度学习训练集噪声，泛化能力差。

#### 例题及答案
**例题1（计算题）**：某垃圾邮件检测模型的预测结果如下：真阳性（TP）=80，假阳性（FP）=20，真阴性（TN）=90，假阴性（FN）=10。计算精确率、召回率，并解释结果含义。
**答案**：
1. 精确率：80/(80+20)=0.8（80%），表示预测为垃圾邮件的样本中，80%是真实垃圾邮件；
2. 召回率：80/(80+10)≈0.889（88.9%），表示所有真实垃圾邮件中，88.9%被正确检测出来。

**例题2（概念题）**：以下哪种方法可缓解过拟合？（  ）
A. 增加训练数据  B. 增大模型复杂度  C. 减少正则化强度  D. 移除验证集
**答案**：A  
解析：增加训练数据可提升模型泛化能力；B、C会加剧过拟合；D无法缓解过拟合，验证集用于检测过拟合。

### 四、人工智能常见应用领域
#### 核心知识点扩展
- **自然语言理解（NLU）**：让模型理解人类语言的含义，如情感分析、语义检索、机器翻译。
- **计算机视觉（CV）**：让模型“看懂”图像/视频，如目标检测、图像分割、人脸识别。
- **智能检索**：基于用户查询快速匹配相关信息，如搜索引擎、文档检索系统。
- **推荐系统**：根据用户偏好推荐个性化内容，如电商商品推荐、视频平台剧集推荐。

#### 例题及答案
**例题（简答题）**：列举自然语言理解和计算机视觉的典型应用场景各2个。
**答案**：
- 自然语言理解：① 社交媒体情感分析（判断用户评论是正面/负面）；② 智能客服（理解用户问题并给出解答）；
- 计算机视觉：① 自动驾驶中的目标检测（识别行人、车辆、交通信号灯）；② 医疗影像诊断（检测X光片中的病灶）。

---

## 第二章 大模型基础概念
### 一、自然语言的基础概念
#### 核心知识点扩展
- **语言的定义**：人类交流的符号系统，通过语音、文字等形式传递意义，具备规则性（语法）、歧义性（多义词）、上下文依赖性。
- **语言的特点**：① 任意性（符号与意义无必然联系）；② 创造性（可生成无限新句子）；③ 社会性（在社会群体中约定俗成）。
- **语言的应用**：信息传递、思维表达、文化传承、人际互动。
- **自然语言与其他信息的区别**：
  - 与图像：语言是抽象符号，图像是视觉具象信息；
  - 与声音：语言是有意义的符号组合，声音可无语义（如噪音）；
  - 与代码：语言具备歧义性和灵活性，代码是严格语法的指令，无歧义。

#### 例题及答案
**例题（简答题）**：简述自然语言与代码的核心区别。
**答案**：① 语法规则：自然语言语法灵活，允许歧义；代码语法严格，无歧义，语法错误会导致无法执行；② 用途：自然语言用于人类交流，传递复杂语义和情感；代码用于控制计算机，执行特定任务；③ 灵活性：自然语言可根据语境调整含义，代码含义固定，依赖明确指令。

### 二、大语言模型的基本定义
#### 核心知识点扩展
- **大语言模型（LLM）**：基于海量文本数据预训练，参数规模庞大（通常数十亿至千亿级），能理解和生成人类语言的模型，核心基于Transformer架构。
- **核心范式**：
  - 生成范式：根据输入文本生成连贯、相关的输出，如文本续写、聊天机器人；
  - 理解范式：对输入文本进行分析处理，如情感分类、文本摘要、语义匹配。
- **与传统NLP模型的区别**：
  - 数据规模：LLM使用海量无标注文本，传统模型依赖少量标注数据；
  - 泛化能力：LLM可迁移到多种任务，传统模型通常针对单一任务优化；
  - 模型规模：LLM参数数十亿级，传统模型参数数百万至数千万级；
- **大模型的优势**：① 强泛化能力，无需大量微调即可适配新任务；② 理解复杂上下文，支持长文本交互；③ 生成自然流畅的语言；
- **大模型的局限性**：① 易产生幻象（生成虚假信息）；② 对专业领域知识的准确性有限；③ 缺乏真正的逻辑推理能力。

#### 例题及答案
**例题1（概念题）**：大语言模型的核心架构基础是（  ）
A. 循环神经网络（RNN）  B. 卷积神经网络（CNN）  C. Transformer  D. 决策树
**答案**：C  
解析：当前主流大语言模型（如GPT、DeepSeek）均基于Transformer架构，其自注意力机制能有效捕捉文本上下文依赖。

**例题2（简答题）**：简述大语言模型的生成范式与理解范式的区别，并各举1个应用场景。
**答案**：① 生成范式：核心是“创造文本”，输入为提示（Prompt），输出为连贯文本，应用场景如小说续写；② 理解范式：核心是“分析文本”，输入为文本和任务指令，输出为分析结果，应用场景如新闻摘要生成。

### 三、发展历程与现状
#### 核心知识点扩展
- **四代语言模型发展历程**：
  1. 统计语言模型：基于概率统计（如n-gram），通过计算词序列概率生成文本，缺点是无法捕捉长距离依赖；
  2. 神经网络语言模型（NNLM）：基于RNN、LSTM等架构，可捕捉部分上下文依赖，但长文本处理能力有限；
  3. 预训练语言模型：基于Transformer架构，先在海量数据上预训练，再通过微调适配特定任务（如BERT、GPT-1）；
  4. 大语言模型：参数规模剧增，预训练数据海量，支持零样本/少样本学习，具备强大的生成和理解能力（如GPT-3、DeepSeek-7B）。
- **关键里程碑**：
  - GPT系列：OpenAI推出，开启生成式大模型时代（GPT-3首次展现千亿参数规模的强泛化能力）；
  - DeepSeek系列：专注于代码生成、科学推理等领域的大模型，支持多语言和复杂任务；
  - BERT：Google推出，基于编码器架构，在理解类任务中表现优异。

#### 例题及答案
**例题（填空题）**：语言模型的发展历程可分为统计语言模型、______、预训练语言模型和______四个阶段，当前主流大模型均基于______架构。
**答案**：神经网络语言模型；大语言模型；Transformer

### 四、扩展法则
#### 核心知识点扩展
- **KM扩展法则**：模型性能（困惑度）随参数量（N）和计算量（C）的增长遵循幂律分布，公式简化为：性能 ∝ N^0.25 * C^0.75，即计算量对性能的影响更大。
- **Chinchilla扩展法则**：在固定计算量下，模型最优性能的条件是“参数量与训练数据量成正比”，具体为：参数量增加k倍时，训练数据量也需增加k倍，才能达到最优效率。
- **核心应用**：指导大模型的设计与训练，避免盲目增加参数量而忽略数据量，或反之，以最优成本实现性能提升。

#### 例题及答案
**例题（计算题）**：根据Chinchilla扩展法则，若某大模型初始参数量为10B，训练数据量为1T tokens，当参数量提升至40B（增加4倍）时，最优训练数据量应调整为多少？
**答案**：4T tokens  
解析：Chinchilla法则要求参数量与训练数据量成正比，参数量增加4倍，训练数据量也需增加4倍，即1T * 4 = 4T tokens。

### 五、大模型代表能力
#### 核心知识点扩展
- **上下文学习（ICL）**：无需微调，仅通过在提示中加入少量示例（少样本）或无示例（零样本），模型即可完成新任务。
- **指令微调（Instruction Tuning）**：通过在微调数据中加入“指令-响应”对，让模型理解并遵循人类指令，提升任务适配性。
- **逐步推理（Step-by-Step Reasoning）**：模型能将复杂任务分解为多个步骤，逐步推导得出结果，如数学计算、逻辑推理。
- **涌现能力**：当模型参数量达到一定阈值（通常数十亿级）后，出现的低参数量模型不具备的能力（如复杂推理、多任务迁移），其本质是模型在海量数据中学习到的深层规律。

#### 例题及答案
**例题（实操题）**：请设计一个零样本上下文学习的提示，让大模型判断句子“这部电影的剧情紧凑，演员演技精湛”的情感倾向。
**答案**：提示：“判断以下句子的情感倾向（正面/负面/中性）：这部电影的剧情紧凑，演员演技精湛”  
模型预期输出：正面  
解析：零样本上下文学习无需额外示例，直接通过指令让模型完成任务，体现大模型的泛化能力。

---

## 第三章 模型架构
### 一、注意力机制
#### 核心知识点扩展
- **核心思想**：模拟人类注意力，在处理文本时，对不同词元赋予不同权重，重点关注与当前词元相关的内容。
- **查询（Query）-键（Key）-值（Value）机制**：
  1. Query：当前词元的特征向量，用于查询相关信息；
  2. Key：所有词元的特征向量，用于与Query匹配；
  3. Value：所有词元的特征向量，用于生成注意力输出；
  4. 计算流程：先计算Query与每个Key的相似度（如点积），通过softmax归一化得到权重，再用权重加权求和Value，得到当前词元的注意力输出。
- **softmax函数**：将相似度分数转换为0-1之间的概率权重，公式：softmax(x_i) = e^x_i / Σe^x_j，确保权重总和为1。
- **自注意力机制（Self-Attention）**：Query、Key、Value均来自同一输入序列，用于捕捉序列内部的依赖关系（如文本中前后词的关联）。
- **交叉注意力机制（Cross-Attention）**：Query来自一个序列，Key和Value来自另一个序列，用于跨序列交互（如机器翻译中，解码器关注编码器的输入文本）。

#### 例题及答案
**例题（计算题）**：已知Query向量Q=[1,2]，Key向量K=[[3,4],[5,6]]，Value向量V=[[7,8],[9,10]]，计算注意力输出（忽略缩放因子）。
**答案**：
1. 计算相似度（点积）：Q·K1=1*3+2*4=11，Q·K2=1*5+2*6=17；
2. softmax归一化：权重w1=e^11/(e^11+e^17)≈0，w2=e^17/(e^11+e^17)≈1；
3. 加权求和Value：w1*V1 + w2*V2 ≈ [9,10]；
最终注意力输出：[9,10]

### 二、主流架构
#### 核心知识点扩展
- **编码器-解码器架构（Encoder-Decoder）**：
  - 结构：包含编码器（处理输入序列，捕捉上下文信息）和解码器（生成输出序列，结合编码器信息和自身历史输出）；
  - 适用任务：序列到序列（Seq2Seq）任务，如机器翻译、文本摘要。
- **因果解码器架构（Causal Decoder）**：
  - 结构：仅包含解码器，且在计算注意力时，只能关注当前词元之前的词元（因果掩码），避免未来信息泄露；
  - 适用任务：生成式任务，如文本续写、聊天机器人（如GPT系列）。
- **前缀解码器架构（Prefix Decoder）**：
  - 结构：结合编码器和解码器的特点，输入序列分为前缀（可双向注意力）和生成部分（因果注意力）；
  - 适用任务：兼具理解和生成的任务，如开放域问答、多轮对话。
- **架构选择与任务类型的关系**：
  - 理解类任务（如情感分类、语义匹配）：优先选择编码器架构（如BERT），能充分捕捉上下文信息；
  - 生成类任务（如文本生成、代码生成）：优先选择因果解码器架构（如GPT），确保生成序列的连贯性；
  -  Seq2Seq任务（如翻译、摘要）：优先选择编码器-解码器架构（如T5）。

#### 例题及答案
**例题（选择题）**：以下任务中，最适合采用因果解码器架构的是（  ）
A. 中文到英文的翻译  B. 新闻文本分类  C. 小说续写  D. 医疗影像分割
**答案**：C  
解析：小说续写是生成式任务，因果解码器架构能确保生成序列的连贯性；A适合编码器-解码器架构，B适合编码器架构，D属于计算机视觉任务，不适用语言模型架构。

### 三、Transformer模型的基本结构组成
#### 核心知识点扩展
- **输入编码**：将文本词元转换为嵌入向量（Embedding），结合词频、位置等信息。
- **位置编码（Positional Encoding）**：
  - 作用：Transformer无循环结构，需通过位置编码注入词元的位置信息；
  - 常见类型：绝对位置编码（如正弦/余弦函数）、相对位置编码（捕捉词元间的相对距离）、旋转位置编码（RoPE，通过旋转矩阵注入位置信息，支持长文本）。
- **多头自注意力机制（Multi-Head Attention）**：
  - 作用：将注意力机制分为多个“头”，每个头关注不同的依赖关系，提升模型表达能力；
  - 流程：将Q、K、V分割为多个子向量，每个子向量独立计算注意力，最后拼接输出。
- **前馈网络层（Feed-Forward Network, FFN）**：
  - 结构：两层全连接网络，中间通过激活函数（如GELU）引入非线性；
  - 作用：对多头注意力的输出进行非线性变换，增强模型的拟合能力。
- **编码器**：由N个编码器层堆叠而成，每个编码器层包含多头自注意力机制和前馈网络层，支持双向注意力。
- **解码器**：由N个解码器层堆叠而成，每个解码器层包含多头自注意力机制（因果掩码）、交叉注意力机制和前馈网络层。

#### 例题及答案
**例题（代码实现题）**：用PyTorch实现Transformer的多头自注意力机制简化版。
```python
import torch
import torch.nn as nn

class MultiHeadAttention(nn.Module):
    def __init__(self, d_model=512, num_heads=8):
        super().__init__()
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads  # 每个头的维度
        
        # 线性投影层
        self.w_q = nn.Linear(d_model, d_model)
        self.w_k = nn.Linear(d_model, d_model)
        self.w_v = nn.Linear(d_model, d_model)
        self.w_o = nn.Linear(d_model, d_model)
    
    def forward(self, q, k, v, mask=None):
        batch_size = q.size(0)
        
        # 投影并分割为多头：(batch_size, seq_len, d_model) → (batch_size, num_heads, seq_len, d_k)
        q = self.w_q(q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        k = self.w_k(k).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        v = self.w_v(v).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        
        # 计算注意力权重：(batch_size, num_heads, seq_len_q, seq_len_k)
        scores = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float32))
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        attn_weights = torch.softmax(scores, dim=-1)
        
        # 加权求和并拼接：(batch_size, num_heads, seq_len_q, d_k) → (batch_size, seq_len_q, d_model)
        output = torch.matmul(attn_weights, v)
        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)
        output = self.w_o(output)
        
        return output

# 测试
model = MultiHeadAttention()
q = torch.randn(2, 10, 512)  # (batch_size, seq_len, d_model)
k = v = q
output = model(q, k, v)
print(output.shape)  # 输出：torch.Size([2, 10, 512])
```
**答案解析**：该代码实现了多头自注意力的核心流程，包括线性投影、多头分割、相似度计算、softmax归一化、加权求和与拼接，最终输出与输入维度一致的特征向量。

### 四、Transformer模型的详细配置（【2】级知识点）
#### 核心知识点扩展
- **归一化方法**：
  - LayerNorm：对每个样本的每个特征层进行归一化，计算简单，训练稳定，是Transformer的主流选择；
  - RMSNorm：基于均方根的归一化，计算量更小，在部分大模型（如LLaMA）中使用，性能与LayerNorm接近。
- **归一化模块位置**：
  - Pre-Norm：归一化在多头注意力和前馈网络之前，训练稳定性更强，当前主流；
  - Post-Norm：归一化在多头注意力和前馈网络之后，早期Transformer使用，易梯度消失。
- **激活函数**：
  - GELU：高斯误差线性单元，公式：GELU(x) = 0.5x(1 + erf(x/√2))，平滑非线性，适合深层模型；
  - Swish：公式：Swish(x) = x·σ(x)（σ为sigmoid函数），自门控特性，部分模型中性能优于GELU。
- **大模型可解释性分析方法**：
  - 探针技术（Probe）：训练简单模型（如线性分类器）从Transformer特征中提取特定信息（如语法结构），验证特征表达；
  - 注意力可视化：将注意力权重以热力图形式展示，直观观察模型关注的词元；
  - 特征重要性分析：通过删除或扰动特征，评估其对模型输出的影响，识别关键特征。

#### 例题及答案
**例题（简答题）**：简述Pre-Norm与Post-Norm的区别及Pre-Norm的优势。
**答案**：① 位置区别：Pre-Norm将归一化放在多头注意力和前馈网络之前，Post-Norm放在之后；② 优势：Pre-Norm能有效缓解深层模型的梯度消失问题，训练稳定性更强，允许堆叠更多层（如千亿参数模型通常采用Pre-Norm），且收敛速度更快。

### 五、长上下文模型（【3】级知识点）
#### 核心知识点扩展
- **位置编码拓展**：
  - ALiBi（Attention with Linear Biases）：不直接注入位置编码，而是在注意力分数中加入线性偏置，随词元距离增大偏置减小，支持任意长度文本；
  - 旋转位置编码（RoPE）：通过旋转矩阵将位置信息融入词向量，满足平移不变性，支持长上下文扩展。
- **长上下文窗口拓展**：
  - 方法：增大模型的上下文窗口长度（如从2k扩展到32k），但需解决显存和计算量问题；
  - 挑战：长文本会导致注意力计算复杂度呈平方增长（O(n²)），显存占用剧增。
- **长上下文数据构建**：
  - 收集长文本数据（如书籍、论文、长文档），进行分段、拼接处理，确保训练数据与目标场景的上下文长度匹配；
  - 数据增强：对长文本进行打乱、截断、填充等操作，提升模型对不同长度文本的适应性。
- **长上下文模型的优化**：
  - 显存优化：采用梯度检查点、模型并行等技术，减少长文本处理时的显存占用；
  - 计算效率优化：使用稀疏注意力、线性注意力等高效架构，降低计算复杂度。

#### 例题及答案
**例题（概念题）**：以下哪种位置编码方法支持任意长度文本，且无需预先定义最大上下文长度？（  ）
A. 正弦位置编码  B. 旋转位置编码（RoPE）  C. ALiBi  D. 相对位置编码
**答案**：C  
解析：ALiBi通过线性偏置注入位置信息，无需预先定义位置编码向量，支持任意长度文本；A、B、D均需预先定义最大长度，超过后无法处理。

---

## 第四章 预训练技术
### 一、回归分析
#### 核心知识点扩展
- **极大似然学习**：
  - 核心思想：选择使观测数据出现概率最大的模型参数，是预训练中常用的参数估计方法；
  - 应用：在语言建模任务中，极大似然估计通过最大化文本序列的生成概率，优化模型参数。
- **多目标分类学习任务**：
  - 定义：模型需同时预测多个相互独立的分类目标（如同时预测文本的情感倾向和主题类别）；
  - 损失函数：通常采用多个分类损失的加权和（如交叉熵损失之和）。

#### 例题及答案
**例题（简答题）**：简述极大似然学习在大模型预训练中的应用。
**答案**：大模型预训练的核心任务是语言建模（如预测下一个词元），极大似然学习通过最大化训练文本序列的生成概率来优化模型参数；具体来说，模型计算每个词元的生成概率，极大似然估计选择使所有词元生成概率乘积最大的参数，确保模型能学习到文本的语言规律（如语法、语义、逻辑）。

### 二、自监督学习
#### 核心知识点扩展
- **对比学习**：
  - 核心思想：通过构建正负样本对，让模型学习区分相似样本（正样本）和不相似样本（负样本），提升特征表达能力；
  - 应用：在文本预训练中，可将同一文本的不同片段作为正样本，不同文本的片段作为负样本。
- **掩码预测（Masked Prediction）**：
  - 核心思想：随机掩盖输入文本中的部分词元，让模型预测被掩盖的词元，强制模型学习上下文依赖；
  - 典型应用：BERT的MLM（Masked Language Model）任务，随机掩盖15%的词元，让模型预测原词。
- **自回归（Autoregressive）**：
  - 核心思想：模型基于前文词元预测下一个词元，生成序列时只能依赖历史信息；
  - 典型应用：GPT系列的预训练任务，通过预测下一个词元（Next Token Prediction）优化模型。
- **自然语言与图像模态自监督学习的区别**：
  - 自然语言：基于文本的序列结构，任务多为掩码预测、自回归生成；
  - 图像：基于像素的空间结构，任务多为图像重构、对比学习（如MoCo、SimCLR）。

#### 例题及答案
**例题（选择题）**：以下属于自监督学习任务的是（  ）
A. 基于标注数据的文本分类  B. BERT的MLM任务  C. 基于人工标注的图像分割  D. 有监督的机器翻译
**答案**：B  
解析：MLM任务通过随机掩码词元并预测，无需人工标注，属于自监督学习；A、C、D均依赖标注数据，属于有监督学习。

### 三、监督学习
#### 核心知识点扩展
- **监督学习**：模型从标注数据（输入x + 输出y）中学习映射关系f(x)→y，训练过程中有明确的“监督信号”（真实标签）。
- **无监督学习**：模型从无标注数据中学习数据的内在规律，无明确的监督信号，如聚类、降维。
- **监督学习与自监督学习的区别与联系**：
  - 区别：监督学习依赖人工标注数据，自监督学习通过数据本身构建监督信号（无需人工标注）；
  - 联系：两者均属于参数学习，通过优化损失函数调整模型参数；自监督学习可视为监督学习的一种特殊形式（监督信号来自数据本身）。

#### 例题及答案
**例题（简答题）**：简述自监督学习相比传统监督学习在大模型预训练中的优势。
**答案**：① 数据成本低：自监督学习无需人工标注，可利用海量无标注文本（如互联网文本），解决了传统监督学习标注数据稀缺、成本高的问题；② 泛化能力强：自监督学习学习的是数据的通用规律（如语言结构），而非特定任务的标注信息，泛化到多种下游任务的能力更强；③ 适配大模型规模：大模型需要海量数据训练，自监督学习能提供足够的训练数据，支撑模型参数的优化。

### 四、预训练任务
#### 核心知识点扩展
- **下一个词元预测（Next Token Prediction, NTP）**：
  - 任务：给定前文词元序列，预测下一个词元的概率分布；
  - 损失函数：交叉熵损失，最小化模型预测概率与真实词元的差距；
  - 代表模型：GPT系列、DeepSeek系列。
- **去噪自编码（Denoising Autoencoder）**：
  - 任务：对输入文本进行“加噪”（如掩码、打乱、替换），让模型还原原始文本；
  - 典型应用：BERT的MLM（掩码语言模型）、T5的Span Masking（掩码连续片段）。
- **下N个词元预测（Next N Token Prediction）**：
  - 任务：给定前文，预测未来N个连续词元，提升模型对长序列依赖的捕捉能力；
  - 优势：相比下一个词元预测，能更好地学习短语、句子级别的语义和逻辑。
- **预训练与多任务学习的关系**：
  - 预训练是多任务学习的一种形式：预训练阶段模型学习多种隐含任务（如语言规律、语义理解）；
  - 多任务预训练：在预训练中融入多种显式任务（如文本分类、问答），提升模型对特定任务的适配性。

#### 例题及答案
**例题（实操题）**：以“人工智能技术正在快速发展”为例，说明GPT系列的预训练任务（下一个词元预测）的工作过程。
**答案**：
1. 输入序列：“人工智能技术正在快速发展”；
2. 模型任务：给定前文“人工智能技术正在快速”，预测下一个词元“发展”；
3. 训练过程：模型计算“发展”在当前上下文下的预测概率，通过交叉熵损失调整参数，使预测概率最大化；
4. 扩展：对于长序列，模型会依次预测每个词元（如“人工”→“智能”→“技术”→…→“发展”），逐步学习文本的生成规律。

### 五、优化设置：基于批次数据的训练方法
#### 核心知识点扩展
- **Batch及Batch Size**：
  - Batch：一次训练中输入模型的样本集合；
  - Batch Size：每个Batch包含的样本数量（如32、64、128）。
- **Batch Size设置对模型训练的影响**：
  - 优点：增大Batch Size可利用硬件并行计算，提升训练速度；减少训练方差，使训练更稳定；
  - 缺点：Batch Size过大会导致显存不足；可能使模型陷入局部最优，泛化能力下降；学习率需同步调整（Batch Size增大时，学习率通常需按比例增大）。
- **动态批次大小调整策略**：
  - 基于显存自动调整：训练时检测显存占用，若显存充足则增大Batch Size，若不足则减小；
  - 基于训练阶段调整：训练初期用小Batch Size快速收敛，后期用大Batch Size稳定训练。

#### 例题及答案
**例题（简答题）**：当Batch Size从32增大到64时，为保证训练效果，通常需要调整哪个超参数？为什么？
**答案**：需要按比例增大学习率（如从1e-4增大到2e-4）；原因：Batch Size增大时，每个Batch的梯度估计更准确，梯度方差减小，为了保持训练步长的有效性，需同步增大学习率，避免训练速度过慢或收敛到局部最优。

### 六、优化设置：学习率
#### 核心知识点扩展
- **学习率（Learning Rate）**：控制模型参数更新的步长，是影响训练效果的关键超参数。
- **学习率衰减（退火）**：
  - 核心思想：训练初期用较大学习率快速探索参数空间，后期用较小学习率精细调整参数，避免震荡；
  - 常见方式：阶梯衰减（Step Decay，每隔一定epoch减小学习率）、指数衰减（Exponential Decay，学习率随epoch指数下降）。
- **学习率预热（Warm-up）**：
  - 核心思想：训练初期用极小学习率逐步增大到目标学习率，避免初期大学习率导致参数震荡；
  - 适用场景：大模型训练（如千亿参数模型），有效稳定初期训练。
- **自适应学习率方法**：
  - Cosine衰减：学习率随训练步数按余弦函数变化，后期学习率缓慢下降，提升泛化能力；
  - 其他方法：Adam的自适应学习率（根据参数梯度调整步长）、LAMB（适用于大Batch Size训练的自适应学习率优化器）。

#### 例题及答案
**例题（计算题）**：某模型的初始学习率为1e-4，采用Cosine衰减，训练总步数为10000步，Warm-up步数为1000步。请问Warm-up结束时的学习率和训练结束时的学习率分别是多少？
**答案**：
1. Warm-up结束时：学习率从0线性增长到目标学习率1e-4，因此结束时学习率为1e-4；
2. 训练结束时：Cosine衰减公式为lr = lr0 * (1 + cos(π * step / total_step)) / 2，当step=10000时，cos(π*10000/10000)=cos(π)=-1，因此lr=1e-4*(1-1)/2=0。

### 七、优化设置：优化器
#### 核心知识点扩展
- **Adam**：结合动量（Momentum）和自适应学习率，收敛速度快，稳定性强，是大模型训练的常用优化器；
- **SGD（随机梯度下降）**：每次用单个样本的梯度更新参数，泛化能力强，但收敛速度慢，需手动调整学习率；
- **优化器的改进版本**：
  - AdamW：在Adam基础上加入权重衰减（Weight Decay），有效缓解过拟合，是当前大模型训练的主流优化器；
  - LAMB：支持大Batch Size训练，自适应调整每个参数的学习率，适用于千亿参数模型的分布式训练。

#### 例题及答案
**例题（选择题）**：以下哪种优化器最适合大模型的预训练任务，且能有效缓解过拟合？（  ）
A. SGD  B. Adam  C. AdamW  D. LAMB
**答案**：C  
解析：AdamW在Adam的基础上加入了权重衰减，能有效控制模型复杂度，缓解过拟合，且收敛速度快，是大模型预训练的首选优化器；A收敛慢，B无权重衰减，D主要适用于大Batch Size训练场景。

### 八、参数量计算（【2】级知识点）
#### 核心知识点扩展
- **Transformer模型参数量计算**：
  - 嵌入层：词汇表大小（V）× 嵌入维度（d_model）；
  - 多头自注意力层：4×d_model²（Q、K、V投影层各d_model²，输出投影层d_model²）；
  - 前馈网络层：2×d_model×d_ff（d_ff为前馈网络隐藏层维度，通常为4d_model）；
  - 总参数量：嵌入层 + 编码器/解码器层数×（多头自注意力层 + 前馈网络层）+ 输出层（d_model×V）。
- **MoE模型的参数量计算**：
  - MoE（混合专家模型）：包含多个专家网络（Expert）和一个门控网络（Gating）；
  - 总参数量 = 门控网络参数量 + 专家网络参数量×专家数量；
  - 有效参数量：训练时每个样本仅激活部分专家（如1-2个），有效参数量远小于总参数量。

#### 例题及答案
**例题（计算题）**：计算一个单解码器Transformer模型的参数量，已知：词汇表大小V=32000，嵌入维度d_model=512，解码器层数L=12，前馈网络隐藏层维度d_ff=2048，无输出层（预训练阶段共享嵌入层）。
**答案**：
1. 嵌入层参数量：32000×512=16,384,000；
2. 单解码器层参数量：
   - 多头自注意力层：4×512²=4×262,144=1,048,576；
   - 前馈网络层：2×512×2048=2,097,152；
   - 单解码器层总参数量：1,048,576 + 2,097,152=3,145,728；
3. 12层解码器参数量：12×3,145,728=37,748,736；
4. 总参数量：16,384,000 + 37,748,736=54,132,736（约5400万）。

### 九、训练运算量及时间计算（【3】级知识点）
#### 核心知识点扩展
- **训练运算量（FLOPs）计算**：
  - FLOPs（浮点运算次数）：衡量模型训练的计算复杂度，单位为FLOP（10^9 FLOPs = 1 GFLOPs）；
  - Transformer训练FLOPs公式（简化）：3×L×d_model²×(2×N + d_ff)，其中L为层数，N为上下文长度；
  - 关键影响因素：模型层数、参数量、上下文长度、训练数据量。
- **训练时间估算**：
  - 训练时间 = 总FLOPs /（硬件算力 × 并行效率）；
  - 硬件算力：GPU的FP16算力（如A100的FP16算力为312 TFLOPs）；
  - 并行效率：分布式训练时，受通信开销影响，并行效率通常为0.7-0.9。

#### 例题及答案
**例题（计算题）**：某Transformer模型的训练总FLOPs为1e18，使用8张A100 GPU（单卡FP16算力312 TFLOPs），并行效率为0.8，估算训练时间（单位：小时）。
**答案**：
1. 总硬件算力：8×312 TFLOPs = 2496 TFLOPs = 2.496×10^12 FLOPs/s；
2. 有效算力：2.496×10^12 × 0.8 = 1.9968×10^12 FLOPs/s；
3. 总训练时间（秒）：1e18 / 1.9968×10^12 ≈ 500,800 秒；
4. 转换为小时：500,800 / 3600 ≈ 139 小时（约5.8天）。

### 十、并行训练（【3】级知识点）
#### 核心知识点扩展
- **3D并行**：
  - 数据并行（Data Parallelism）：将训练数据拆分到多个GPU，每个GPU训练完整模型，通过梯度同步更新参数；
  - 流水线并行（Pipeline Parallelism）：将模型层拆分到多个GPU，每个GPU负责部分层的计算，通过流水线方式提升效率；
  - 张量并行（Tensor Parallelism）：将模型层的张量（如权重矩阵）拆分到多个GPU，并行计算矩阵乘法等操作，适用于超大参数量模型。
- **ZeRO优化器（Zero Redundancy Optimizer）**：
  - 核心思想：将模型参数、梯度、优化器状态在多个GPU间拆分存储，避免冗余，减少显存占用；
  - 级别：ZeRO-1（拆分优化器状态）、ZeRO-2（拆分梯度和优化器状态）、ZeRO-3（拆分参数、梯度和优化器状态）。
- **混合并行策略**：
  - 结合3D并行和ZeRO优化，如数据并行+张量并行+ZeRO-3，同时解决数据量和参数量过大的问题，是千亿参数模型训练的常用策略。

#### 例题及答案
**例题（简答题）**：简述张量并行与数据并行的核心区别及适用场景。
**答案**：① 核心区别：数据并行拆分训练数据，每个GPU存储完整模型；张量并行拆分模型张量，每个GPU存储部分模型参数；② 适用场景：数据并行适用于数据量较大、参数量适中的模型；张量并行适用于参数量极大（如千亿级）的模型，可有效降低单卡显存占用。
